{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import ast\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import model\n",
    "import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fails(func):\n",
    "    try:\n",
    "        func()\n",
    "        return False\n",
    "    except:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.read_all_anytask()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n"
     ]
    }
   ],
   "source": [
    "for handle, result_for_handle in data.items():\n",
    "    current = {}\n",
    "    for problem, solution in result_for_handle.items():\n",
    "#         if problem in common_problems:\n",
    "            if not fails(lambda: ast.parse(solution)):\n",
    "                current[problem] = solution\n",
    "            else:\n",
    "                print(\"Failed\")\n",
    "    \n",
    "    data[handle] = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_people(df, people):\n",
    "    result = {}\n",
    "    for handle, result_for_handle in df.items():\n",
    "        if handle in people:\n",
    "            result[handle] = result_for_handle\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_ast_size(df, mn=0, mx=500):\n",
    "    result = {}\n",
    "    for handle, result_for_handle in df.items():\n",
    "        current_result = {}\n",
    "        for problem, submission in result_for_handle.items():\n",
    "            try:\n",
    "                parsed = ast.parse(submission)\n",
    "                length = len(list(ast.walk(parsed)))\n",
    "                print(length)\n",
    "                if length >= mn and length <= mx:\n",
    "                    current_result[problem] = submission\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        result[handle] = current_result\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n",
      "2352\n",
      "1194\n",
      "250\n",
      "49\n",
      "176\n",
      "67\n",
      "51\n",
      "83\n",
      "639\n",
      "35\n",
      "62\n",
      "183\n",
      "1881\n",
      "639\n",
      "543\n",
      "582\n",
      "174\n",
      "709\n",
      "139\n",
      "1532\n",
      "134\n",
      "83\n",
      "233\n",
      "115\n",
      "447\n",
      "35\n",
      "67\n",
      "11171\n",
      "87\n",
      "20\n",
      "571\n",
      "62\n",
      "168\n",
      "127\n",
      "77\n",
      "149\n",
      "1257\n",
      "1755\n",
      "660\n",
      "223\n",
      "705\n",
      "4399\n",
      "1927\n",
      "73\n",
      "255\n",
      "66\n",
      "719\n",
      "148\n",
      "577\n",
      "92\n",
      "494\n",
      "247\n",
      "671\n",
      "831\n",
      "41\n",
      "642\n",
      "951\n",
      "1004\n",
      "1654\n",
      "186\n",
      "76\n",
      "372\n",
      "93\n",
      "1772\n",
      "169\n",
      "181\n",
      "46\n",
      "1691\n",
      "475\n",
      "237\n",
      "469\n",
      "1155\n",
      "1689\n",
      "115\n",
      "884\n",
      "108\n",
      "143\n",
      "96\n",
      "100\n",
      "41\n",
      "105\n",
      "29\n",
      "209\n",
      "20\n",
      "73\n",
      "411\n",
      "160\n",
      "632\n",
      "614\n",
      "32\n",
      "1348\n",
      "396\n",
      "146\n",
      "55\n",
      "68\n",
      "128\n",
      "20\n",
      "67\n",
      "260\n",
      "1677\n",
      "491\n",
      "586\n",
      "584\n",
      "83\n",
      "542\n",
      "73\n",
      "1562\n",
      "69\n",
      "139\n",
      "200\n",
      "99\n",
      "443\n",
      "47\n",
      "40\n",
      "116\n",
      "205\n",
      "134\n",
      "83\n",
      "55\n",
      "70\n",
      "120\n",
      "85\n",
      "257\n",
      "18\n",
      "39\n",
      "595\n",
      "208\n",
      "1331\n",
      "44\n",
      "668\n",
      "1259\n",
      "218\n",
      "1243\n",
      "52\n",
      "140\n",
      "63\n",
      "70\n",
      "1255\n",
      "83\n",
      "1579\n",
      "1042\n",
      "20\n",
      "69\n",
      "233\n",
      "415\n",
      "1784\n",
      "1042\n",
      "606\n",
      "873\n",
      "140\n",
      "632\n",
      "129\n",
      "1639\n",
      "50\n",
      "88\n",
      "251\n",
      "227\n",
      "621\n",
      "31\n",
      "576\n",
      "516\n",
      "1389\n",
      "75\n",
      "541\n",
      "681\n",
      "1243\n",
      "83\n",
      "80\n",
      "937\n",
      "53\n",
      "84\n",
      "109\n",
      "531\n",
      "20\n",
      "75\n",
      "675\n",
      "1351\n",
      "590\n",
      "517\n",
      "1007\n",
      "258\n",
      "50\n",
      "69\n",
      "75\n",
      "182\n",
      "20\n",
      "304\n",
      "2124\n",
      "445\n",
      "26\n",
      "653\n",
      "63\n",
      "426\n",
      "89\n",
      "1708\n",
      "79\n",
      "51\n",
      "105\n",
      "10\n",
      "114\n",
      "250\n",
      "127\n",
      "62\n",
      "20\n",
      "62\n",
      "570\n",
      "225\n",
      "91\n",
      "78\n",
      "47\n",
      "717\n",
      "1985\n",
      "837\n",
      "213\n",
      "85\n",
      "54\n",
      "72\n",
      "100\n",
      "746\n",
      "25\n",
      "107\n",
      "312\n",
      "1180\n",
      "514\n",
      "592\n",
      "763\n",
      "107\n",
      "684\n",
      "99\n",
      "2754\n",
      "119\n",
      "81\n",
      "276\n",
      "131\n",
      "739\n",
      "41\n",
      "1733\n",
      "1584\n",
      "188\n",
      "46\n",
      "72\n",
      "208\n",
      "118\n",
      "20\n",
      "65\n",
      "203\n",
      "1600\n",
      "390\n",
      "395\n",
      "471\n",
      "68\n",
      "1316\n",
      "84\n",
      "1536\n",
      "67\n",
      "87\n",
      "156\n",
      "67\n",
      "354\n",
      "38\n",
      "1243\n",
      "2023\n",
      "1302\n",
      "212\n",
      "165\n",
      "66\n",
      "73\n",
      "347\n",
      "21\n",
      "61\n",
      "213\n",
      "2119\n",
      "891\n",
      "763\n",
      "1033\n",
      "80\n",
      "813\n",
      "122\n",
      "1570\n",
      "72\n",
      "187\n",
      "281\n",
      "629\n",
      "58\n",
      "32\n",
      "2028\n",
      "1126\n",
      "188\n",
      "1285\n",
      "35\n",
      "56\n",
      "59\n",
      "120\n",
      "20\n",
      "62\n",
      "169\n",
      "2113\n",
      "644\n",
      "488\n",
      "655\n",
      "72\n",
      "575\n",
      "57\n",
      "1711\n",
      "96\n",
      "82\n",
      "233\n",
      "64\n",
      "612\n",
      "75\n",
      "1066\n",
      "1014\n",
      "526\n",
      "75\n",
      "176\n",
      "45\n",
      "73\n",
      "127\n",
      "20\n",
      "54\n",
      "234\n",
      "2097\n",
      "389\n",
      "546\n",
      "891\n",
      "118\n",
      "896\n",
      "162\n",
      "1285\n",
      "53\n",
      "129\n",
      "238\n",
      "494\n",
      "39\n",
      "32\n",
      "1890\n",
      "721\n",
      "221\n",
      "63\n",
      "98\n",
      "92\n",
      "79\n",
      "20\n",
      "68\n",
      "186\n",
      "1689\n",
      "437\n",
      "519\n",
      "566\n",
      "117\n",
      "673\n",
      "139\n",
      "1112\n",
      "87\n",
      "85\n",
      "207\n",
      "10\n",
      "514\n",
      "48\n",
      "596\n",
      "1321\n",
      "1240\n",
      "346\n",
      "76\n",
      "445\n",
      "62\n",
      "75\n",
      "153\n",
      "438\n",
      "26\n",
      "52\n",
      "3041\n",
      "438\n",
      "590\n",
      "522\n",
      "103\n",
      "736\n",
      "126\n",
      "1620\n",
      "127\n",
      "86\n",
      "190\n",
      "98\n",
      "699\n",
      "48\n",
      "32\n",
      "1990\n",
      "818\n",
      "226\n",
      "762\n",
      "37\n",
      "210\n",
      "46\n",
      "70\n",
      "100\n",
      "504\n",
      "20\n",
      "57\n",
      "152\n",
      "1500\n",
      "504\n",
      "573\n",
      "562\n",
      "100\n",
      "970\n",
      "1245\n",
      "73\n",
      "2349\n",
      "38\n",
      "96\n",
      "162\n",
      "103\n",
      "901\n",
      "44\n",
      "99\n",
      "781\n",
      "32\n",
      "1851\n",
      "107\n",
      "258\n",
      "82\n",
      "96\n",
      "98\n",
      "142\n",
      "20\n",
      "221\n",
      "87\n",
      "2413\n",
      "442\n",
      "1021\n",
      "541\n",
      "1256\n",
      "1391\n",
      "103\n",
      "79\n",
      "257\n",
      "1421\n",
      "201\n",
      "604\n",
      "45\n",
      "615\n",
      "516\n"
     ]
    }
   ],
   "source": [
    "data = filter_ast_size(data, mn=0, mx=4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import numpy as np\n",
      "import oracles\n",
      "import time\n",
      "\n",
      "\n",
      "from sklearn.base import BaseEstimator\n",
      "\n",
      "\n",
      "class PEGASOSMethod(BaseEstimator):\n",
      "    \"\"\"\n",
      "    Реализация метода Pegasos для решения задачи svm.\n",
      "    \"\"\"\n",
      "    def __init__(self, step_lambda=1, num_iter=10000, batch_size=None, C=10):\n",
      "        \"\"\"\n",
      "        step_lambda - величина шага, соответствует \n",
      "        \n",
      "        batch_size - размер батча\n",
      "        \n",
      "        num_iter - число итераций метода, предлагается делать константное\n",
      "        число итераций \n",
      "        \"\"\"\n",
      "        self.step_lambda = step_lambda\n",
      "        self.batch_size = batch_size\n",
      "        self.num_iter = num_iter\n",
      "        self.C = C\n",
      "        \n",
      "    def fit(self, X, y, tolerance=1e-5, trace=False):\n",
      "        \"\"\"\n",
      "        Обучение метода по выборке X с ответами y\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        trace - переменная типа bool\n",
      "      \n",
      "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
      "        о поведении метода. Длина словаря history = количество итераций + 1 (начальное приближение)\n",
      "        \n",
      "        history['time']: list of floats, содержит интервалы времени между двумя итерациями метода\n",
      "        history['func']: list of floats, содержит значения функции на каждой итерации\n",
      "        (0 для самой первой точки)\n",
      "        \"\"\"\n",
      "        l = X.shape[0]\n",
      "        d = X.shape[1]\n",
      "        if self.batch_size is None:\n",
      "            self.batch_size = l\n",
      "        self.oracle = oracles.BinaryHinge(C=self.C, method='pegasos')\n",
      "        self.w = np.zeros(d)\n",
      "        self.best = self.oracle.func(X, y, self.w)\n",
      "        self.wbest = self.w\n",
      "        history = {'time': [0], 'func': [self.oracle.func(X, y, self.w)]}\n",
      "        for i in range(self.num_iter):\n",
      "            start = time.time()\n",
      "            idx = np.random.permutation(l)\n",
      "            y_idx = y[idx[:self.batch_size]]\n",
      "            elems = X[idx[:self.batch_size]]\n",
      "            scalar = elems.dot(self.w) * y_idx\n",
      "            scalar[scalar <= 1] = 1\n",
      "            scalar[scalar > 1] = 0\n",
      "            alpha = 1 / ((i + 1) * self.step_lambda)\n",
      "            step = 1 - alpha * self.step_lambda\n",
      "            a = (elems * y_idx[:, np.newaxis]) * scalar[:, np.newaxis]\n",
      "            self.w = step * self.w + (alpha / self.batch_size) * a.sum(axis=0)\n",
      "            self.w = np.amin((1, 1 / (np.sqrt(self.step_lambda * np.dot(self.w, self.w))))) * self.w\n",
      "            func = self.oracle.func(X, y, self.w)\n",
      "            history['func'].append(func)\n",
      "            if func < self.best:\n",
      "                self.best = self.oracle.func(X, y, self.w)\n",
      "                self.wbest = self.w\n",
      "            history['time'].append(time.time() - start)\n",
      "            if np.absolute(func - history['func'][-2]) < tolerance:\n",
      "                break\n",
      "        if trace:\n",
      "            return history\n",
      "        \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Получить предсказания по выборке X\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \"\"\"\n",
      "        predict = X.dot(self.wbest)\n",
      "        predict[predict > 0] = 1\n",
      "        predict[predict <= 0] = -1\n",
      "        return predict\n",
      "    \n",
      "    \n",
      "class SubgradientMethod(BaseEstimator):\n",
      "    \"\"\"\n",
      "    Реализация метода субградиентного спуска для решения задачи svm.\n",
      "    \"\"\"\n",
      "    def __init__(self, batch_size=None, step_alpha=10, step_beta=0.5, num_iter=10000, C=10):\n",
      "        \"\"\"\n",
      "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
      "        Может принимать значения:\n",
      "        - 'binary_logistic' - бинарная логистическая регрессия\n",
      "        - 'multinomial_logistic' - многоклассовая логистическая регрессия\n",
      "                \n",
      "        step_alpha - float, параметр выбора шага из текста задания\n",
      "        \n",
      "        step_beta- float, параметр выбора шага из текста задания\n",
      "        \n",
      "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию.\n",
      "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
      "        если (f(x_{k+1}) - f(x_{k})) < tolerance: то выход \n",
      "        \n",
      "        max_iter - максимальное число итераций     \n",
      "        \n",
      "        **kwargs - аргументы, необходимые для инициализации   \n",
      "        \"\"\"\n",
      "        self.batch_size = batch_size\n",
      "        self.step_alpha = step_alpha\n",
      "        self.step_beta = step_beta\n",
      "        self.num_iter = num_iter\n",
      "        self.C = C\n",
      "        \n",
      "    def fit(self, X, y, tolerance=1e-5, trace=False):\n",
      "        \"\"\"\n",
      "        Обучение метода по выборке X с ответами y\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        trace - переменная типа bool\n",
      "      \n",
      "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
      "        о поведении метода. Длина словаря history = количество итераций + 1 (начальное приближение)\n",
      "        \n",
      "        history['time']: list of floats, содержит интервалы времени между двумя итерациями метода\n",
      "        history['func']: list of floats, содержит значения функции на каждой итерации\n",
      "        (0 для самой первой точки)\n",
      "        \"\"\"\n",
      "        l = X.shape[0]\n",
      "        d = X.shape[1]\n",
      "        if self.batch_size is None:\n",
      "            self.batch_size = l\n",
      "        self.oracle = oracles.BinaryHinge(C=self.C, method='subgradient')\n",
      "        self.w = np.ones(d)\n",
      "        self.best = self.oracle.func(X, y, self.w)\n",
      "        self.wbest = self.w\n",
      "        history = {'time': [0], 'func': [self.oracle.func(X, y, self.w)]}\n",
      "        for i in range(self.num_iter):\n",
      "            start = time.time()\n",
      "            idx = np.random.permutation(l)\n",
      "            y_idx = y[idx[:self.batch_size]]\n",
      "            elems = X[idx[:self.batch_size]]\n",
      "            step = self.step_alpha / ((i + 1) ** self.step_beta)\n",
      "            self.w = self.w - step * self.oracle.grad(elems, y_idx, self.w)\n",
      "            func = self.oracle.func(X, y, self.w)\n",
      "            history['func'].append(func)\n",
      "            if func < self.best:\n",
      "                self.best = self.oracle.func(X, y, self.w)\n",
      "                self.wbest = self.w\n",
      "            history['time'].append(time.time() - start)\n",
      "            if np.absolute(func - history['func'][-2]) < tolerance:\n",
      "                break\n",
      "        if trace:\n",
      "            return history\n",
      "        \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Получить предсказания по выборке X\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \"\"\"\n",
      "        predict = X.dot(self.wbest)\n",
      "        predict[predict > 0] = 1\n",
      "        predict[predict <= 0] = -1\n",
      "        return predict \n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "from sklearn.neighbors import NearestNeighbors as NN\n",
      "from scipy.sparse import csr_matrix\n",
      "\n",
      "\n",
      "eps = 10 ** -5\n",
      "\n",
      "\n",
      "def euclidean(X1, X2):\n",
      "    X1_sq_sum = np.sum(X1 ** 2, axis=1)\n",
      "    X2_sq_sum = np.sum(X2 ** 2, axis=1)\n",
      "    product = np.dot(X1, X2.T)\n",
      "    distance_matrix = np.sqrt(X1_sq_sum[:, np.newaxis] + X2_sq_sum - 2 * product)\n",
      "    return distance_matrix\n",
      "\n",
      "\n",
      "def cosine(X1, X2):\n",
      "    distance_matrix = np.empty([X1.shape[0], X2.shape[0]])\n",
      "    X1_norm = np.sum(X1 ** 2, axis=1) ** 0.5\n",
      "    X2_norm = np.sum(X2 ** 2, axis=1) ** 0.5\n",
      "    X1_normalized = X1 / X1_norm[:, np.newaxis]\n",
      "    X2_normalized = X2 / X2_norm[:, np.newaxis]\n",
      "    distance_matrix = 1 - np.dot(X1_normalized, X2_normalized.T)\n",
      "    return distance_matrix\n",
      "\n",
      "\n",
      "class KNNClassifier:\n",
      "    def __init__(self, k=1, strategy='brute', metric='euclidean', weights=False, test_block_size=10000):\n",
      "        self.k = k\n",
      "        self.strategy = 'brute'\n",
      "        self.metric = metric\n",
      "        self.weights = weights\n",
      "        self.test_block_size = test_block_size\n",
      "        if self.strategy != 'my_own':\n",
      "            self.model = NN(algorithm=self.strategy, metric=self.metric)\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        self.y = y\n",
      "        self.len_y = len(y)\n",
      "        self.classes = np.unique(y)\n",
      "        if self.strategy != 'my_own':\n",
      "            self.model.fit(X, y)\n",
      "        else:\n",
      "            if isinstance(X, csr_matrix):\n",
      "                X = X.toarray()\n",
      "            self.X = X\n",
      "            \n",
      "    def find_kneighbors(self, X, return_distance):\n",
      "        if self.strategy != 'my_own':\n",
      "            return self.model.kneighbors(X, self.k, return_distance)\n",
      "        else:\n",
      "            if isinstance(X, csr_matrix):\n",
      "                X = X.toarray()\n",
      "            distance_all = euclidean(X, self.X) if self.metric == 'euclidean' else cosine(X, self.X)\n",
      "            distance = np.empty([X.shape[0], self.k])\n",
      "            indexes = np.empty([X.shape[0], self.k])\n",
      "            for i in range(distance_all.shape[0]):\n",
      "                dist_idx_list = np.array(list(zip(distance_all[i], range(self.len_y))))\n",
      "                partitioned = dist_idx_list[np.argpartition(dist_idx_list.T[0], self.k)][:self.k]\n",
      "                sorted_partitioned = partitioned[np.argsort(partitioned.T[0])]\n",
      "                distance[i] = sorted_partitioned.T[0]\n",
      "                indexes[i] = sorted_partitioned.T[1]\n",
      "            if return_distance:\n",
      "                return distance, indexes.astype(int)\n",
      "            else:\n",
      "                return indexes.astype(int)\n",
      "        \n",
      "    def predict(self, X):\n",
      "        n_blocks = X.shape[0] // self.test_block_size\n",
      "        if X.shape[0] - n_blocks * self.test_block_size != 0:\n",
      "            n_blocks += 1\n",
      "        prediction = []\n",
      "        for k in range(n_blocks):\n",
      "            X_new = X[k * self.test_block_size: min((k + 1) * self.test_block_size, X.shape[0])]\n",
      "            if self.weights:\n",
      "                distance, idx = self.find_kneighbors(X_new, True)\n",
      "                for i in range(X_new.shape[0]):\n",
      "                    dic = dict(zip(self.classes, np.zeros(len(self.classes))))\n",
      "                    for j in range(self.k):\n",
      "                        dic[self.y[idx[i][j]]] += 1 / (distance[i][j] + eps)\n",
      "                    prediction.append(sorted(dic.items(), key=lambda x: - x[1])[0][0])\n",
      "            else:\n",
      "                idx = self.find_kneighbors(X_new, False)\n",
      "                for i in range(X_new.shape[0]):\n",
      "                    dic = dict(zip(self.classes, np.zeros(len(self.classes))))\n",
      "                    for j in range(self.k):\n",
      "                        dic[self.y[idx[i][j]]] += 1\n",
      "                    prediction.append(sorted(dic.items(), key=lambda x: - x[1])[0][0])\n",
      "        return np.array(prediction)\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import scipy\n",
      "\n",
      "\n",
      "class BaseSmoothOracle:\n",
      "    \"\"\"\n",
      "    Базовый класс для реализации оракулов.\n",
      "    \"\"\"\n",
      "    def func(self, w):\n",
      "        \"\"\"\n",
      "        Вычислить значение функции в точке w.\n",
      "        \"\"\"\n",
      "        l = self.X.shape[0]\n",
      "        if self.method == 'pegasos':\n",
      "            scalar = 0.5 * np.dot(w, w)\n",
      "            multi = 1 - self.X.dot(w) * self.y\n",
      "            multi[multi < 0] = 0\n",
      "            return scalar + (self.C / l) * multi.sum()\n",
      "        if self.method == 'subgradient':\n",
      "            scalar = 0.5 * np.dot(w[:-1], w[:-1])\n",
      "            multi = 1 - self.X.dot(w) * self.y\n",
      "            multi[multi < 0] = 0\n",
      "            return scalar + (self.C / l) * multi.sum()\n",
      "\n",
      "    def grad(self, w):\n",
      "        \"\"\"\n",
      "        Вычислить значение градиента функции в точке w.\n",
      "        \"\"\"\n",
      "        l = self.X.shape[0]\n",
      "        if self.method == 'pegasos':\n",
      "            multi = 1 - self.X.dot(w) * self.y\n",
      "            multi[multi <= 0] = 0\n",
      "            multi[multi > 0] = 1\n",
      "            return w - (self.C / l) * (self.X * self.y[:, np.newaxis] * multi[:, np.newaxis]).sum(axis=0)\n",
      "        if self.method == 'subgradient':\n",
      "            multi = 1 - self.X.dot(w) * self.y\n",
      "            multi[multi <= 0] = 0\n",
      "            multi[multi > 0] = 1\n",
      "            new_w = w.copy()\n",
      "            new_w[0] = 0\n",
      "            return new_w - (self.C / l) * (self.X * self.y[:, np.newaxis] * multi[:, np.newaxis]).sum(axis=0)\n",
      "\n",
      "        \n",
      "class BinaryHinge(BaseSmoothOracle):\n",
      "    \"\"\"\n",
      "    Оракул для задачи двухклассового линейного SVM.\n",
      "    \n",
      "    Нулевая координата вектора w соответствует w_0.\n",
      "    Считается, что в классификатор подаётся X с уже созданным единичным столбцом\n",
      "    (так делается для того, чтобы не переписывать код из предыдущего задания).\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, C=10, method='subgradient'):\n",
      "        \"\"\"\n",
      "        Задание параметров оракула.\n",
      "        \n",
      "        C - коэффициент регуляризации в функционале SVM\n",
      "        \"\"\"\n",
      "        self.C = C\n",
      "        self.method = method\n",
      "     \n",
      "    def func(self, X, y, w):\n",
      "        \"\"\"\n",
      "        Вычислить значение функционала в точке w на выборке X с ответами y.\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        w - одномерный numpy array\n",
      "        \"\"\"\n",
      "        self.X = X\n",
      "        self.y = y\n",
      "        return super().func(w)\n",
      "        \n",
      "    def grad(self, X, y, w):\n",
      "        \"\"\"\n",
      "        Вычислить субградиент функционала в точке w на выборке X с ответами y.\n",
      "        Субгрдиент в точке 0 необходимо зафиксировать равным 0.\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        w - одномерный numpy array\n",
      "        \"\"\"\n",
      "        self.X = X\n",
      "        self.y = y\n",
      "        return super().grad(w)\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "def linearize(x):\n",
      "    it = iter(x)\n",
      "    obj = next(it)\n",
      "    while True:\n",
      "        try:\n",
      "            if obj is x:\n",
      "                raise StopIteration\n",
      "            it2 = iter(obj)\n",
      "            yield from linearize(obj)\n",
      "            try:\n",
      "                obj = next(it)\n",
      "            except:\n",
      "                return\n",
      "        except:\n",
      "            yield obj\n",
      "            try:\n",
      "                obj = next(it)\n",
      "            except:\n",
      "                return\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import functools\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def memoized(f=None, *, max_count=float('inf')):\n",
      "    cache = {}\n",
      "    if max_count != float('inf'):\n",
      "        order = [np.nan] * max_count\n",
      "    num = 0\n",
      "    if f is None:\n",
      "        def wrapper_function(f):\n",
      "            return memoized(f, max_count=max_count)\n",
      "        return wrapper_function\n",
      "\n",
      "    @functools.wraps(f)\n",
      "    def wrapper_function(*args, **kwargs):\n",
      "        nonlocal num\n",
      "        key = args + tuple(sorted(kwargs.items()))\n",
      "        if key not in cache:\n",
      "            if max_count != float('inf'):\n",
      "                cache.pop(order[num], None)\n",
      "                order[num] = key\n",
      "                num = (num + 1) % max_count\n",
      "            cache[key] = f(*args, **kwargs)\n",
      "        return cache[key]\n",
      "    return wrapper_function\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def get_nonzero_diag_product(X):\n",
      "    diag_of_X = np.diag(X)\n",
      "    mask = diag_of_X != 0\n",
      "    if np.any(mask):\n",
      "        return diag_of_X[mask].prod()\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "def find_max_substring_occurrence(input_string):\n",
      "    acceptable_len = [i for i in range(1, len(input_string) + 1) if len(input_string) % i == 0]\n",
      "    for i in acceptable_len:\n",
      "        k = len(input_string) // i\n",
      "        if input_string[:i] * k == input_string:\n",
      "            return k\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import functools\n",
      "\n",
      "\n",
      "def substitutive(func):\n",
      "    @functools.wraps(func)\n",
      "    def wrapper_function(*args, **kwargs):\n",
      "        @functools.wraps(func)\n",
      "        def getter(*new_args, **new_kwargs):\n",
      "            if len(set(new_kwargs.keys()) & set(kwargs.keys())) != 0:\n",
      "                raise SyntaxError('keyword argument repeated')\n",
      "            kw = kwargs.copy()\n",
      "            kw.update(new_kwargs)\n",
      "            return wrapper_function(*(args + new_args), **kw)\n",
      "        if func.__code__.co_argcount <= len(args) + len(kwargs):\n",
      "            return func(*args, **kwargs)\n",
      "        else:\n",
      "            return getter\n",
      "    return wrapper_function\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def get_elements_by_indexes(X, i_indexes, j_indexes):\n",
      "    return X[i_indexes, j_indexes]\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def get_max_before_zero(x):\n",
      "    mask = x == 0\n",
      "    mask = np.append([False], mask[:-1])\n",
      "    if mask.any():\n",
      "        return np.amax(x[mask])\n",
      "    else:\n",
      "        return None\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "def get_new_dictionary(input_dict_name, output_dict_name):\n",
      "    import re\n",
      "    new_dict = {}\n",
      "    input_file = open(input_dict_name, 'r')\n",
      "    output_file = open(output_dict_name, 'w')\n",
      "    line1 = input_file.readline()\n",
      "    num_of_words = int(line1)\n",
      "    if num_of_words == 0:\n",
      "        output_file.write(line1)\n",
      "        output_file.close()\n",
      "        input_file.close()\n",
      "    else:\n",
      "        for line in input_file:\n",
      "            tmp_line = line.replace('\\n', '')\n",
      "            item = re.split('\\s-\\s', tmp_line)[0]\n",
      "            keys = re.split(',\\s', re.split('\\s-\\s', tmp_line)[1])\n",
      "            for key in keys:\n",
      "                if key in new_dict.keys():\n",
      "                    new_dict[key] = new_dict[key] + [item]\n",
      "                else:\n",
      "                    new_dict[key] = [item]\n",
      "        output_file.write(str(len(new_dict)) + '\\n')\n",
      "        for key in new_dict.keys():\n",
      "            output_file.write(key + ' - ' + ', '.join(new_dict[key]) + '\\n')\n",
      "        output_file.close()\n",
      "        input_file.close()\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "from cvxopt import matrix\n",
      "from cvxopt import solvers\n",
      "from numpy import linalg\n",
      "from sklearn.base import BaseEstimator\n",
      "\n",
      "\n",
      "def normed_sq(X1, X2):\n",
      "    X1_sq_sum = np.sum(X1 ** 2, axis=1)\n",
      "    X2_sq_sum = np.sum(X2 ** 2, axis=1)\n",
      "    product = np.dot(X1, X2.T)\n",
      "    dist = X1_sq_sum[:, np.newaxis] + X2_sq_sum - 2 * product\n",
      "    dist[dist <= 0] = 0.0\n",
      "    return dist\n",
      "\n",
      "\n",
      "class SVMSolver(BaseEstimator):\n",
      "    \"\"\"\n",
      "    Класс с реализацией SVM через метод внутренней точки.\n",
      "    \"\"\"\n",
      "    def __init__(self, C, method, kernel='linear', gamma=None, degree=None):\n",
      "        \"\"\"\n",
      "        C - float, коэффициент регуляризации\n",
      "        \n",
      "        method - строка, задающая решаемую задачу, может принимать значения:\n",
      "            'primal' - соответствует прямой задаче\n",
      "            'dual' - соответствует двойственной задаче\n",
      "        kernel - строка, задающая ядро при решении двойственной задачи\n",
      "            'linear' - линейное\n",
      "            'polynomial' - полиномиальное\n",
      "            'rbf' - rbf-ядро\n",
      "        gamma - ширина rbf ядра, только если используется rbf-ядро\n",
      "        d - степень полиномиального ядра, только если используется полиномиальное ядро\n",
      "        Обратите внимание, что часть функций класса используется при одном методе решения,\n",
      "        а часть при другом\n",
      "        \"\"\"\n",
      "        self.C = C\n",
      "        self.method = method\n",
      "        self.kernel = kernel\n",
      "        self.gamma = gamma\n",
      "        self.degree = degree\n",
      "    \n",
      "    def compute_primal_objective(self, X, y):\n",
      "        \"\"\"\n",
      "        Метод для подсчета целевой функции SVM для прямой задачи\n",
      "        \n",
      "        X - переменная типа numpy.array, признаковые описания объектов из обучающей выборки\n",
      "        y - переменная типа numpy.array, правильные ответы на обучающей выборке,\n",
      "        \"\"\"\n",
      "        first = np.hstack((X, np.ones(X.shape[0])[:, np.newaxis])) * y[:, np.newaxis]\n",
      "        vec = 1 - np.dot(first, np.append(self.w, self.w0))\n",
      "        vec[vec < 0] = 0\n",
      "        value = 0.5 * np.dot(self.w, self.w) + (self.C / X.shape[0]) * vec.sum()\n",
      "        return value\n",
      "        \n",
      "    def compute_dual_objective(self, X, y):\n",
      "        \"\"\"\n",
      "        Метод для подсчёта целевой функции SVM для двойственной задачи\n",
      "        \n",
      "        X - переменная типа numpy.array, признаковые описания объектов из обучающей выборки\n",
      "        y - переменная типа numpy.array, правильные ответы на обучающей выборке,\n",
      "        \"\"\" \n",
      "        l = X.shape[0]\n",
      "        q = np.ones(l)\n",
      "        if self.kernel == 'linear':\n",
      "            P = np.dot(X, X.T) * y[:, np.newaxis] * y[np.newaxis, :]\n",
      "        elif self.kernel == 'polynomial':\n",
      "            P = ((np.dot(X, X.T) + 1) ** self.degree) * y[:, np.newaxis] * y[np.newaxis, :]\n",
      "        else:\n",
      "            norm2 = normed_sq(X, X)\n",
      "            K = np.exp(-self.gamma * norm2)\n",
      "            P = K * y[:, np.newaxis] * y[np.newaxis, :]\n",
      "        return 0.5 * np.dot(self.dual, np.dot(P, self.dual)) - np.dot(q, self.dual)\n",
      "        \n",
      "    def fit(self, X, y, tolerance, max_iter):\n",
      "        \"\"\"\n",
      "        Метод для обучения svm согласно выбранной в method задаче\n",
      "        \n",
      "        X - переменная типа numpy.array, признаковые описания объектов из обучающей выборки\n",
      "        y - переменная типа numpy.array, правильные ответы на обучающей выборке,\n",
      "        tolerance - требуемая точность для метода обучения\n",
      "        max_iter - максимальное число итераций в методе\n",
      "        \n",
      "        \"\"\"\n",
      "        l = X.shape[0]\n",
      "        d = X.shape[1]\n",
      "        solvers.options['maxiters'] = max_iter\n",
      "        solvers.options['reltol'] = tolerance\n",
      "        if self.method == 'primal':\n",
      "            P = np.zeros((l + d + 1, l + d + 1))\n",
      "            P[range(d), range(d)] = 1\n",
      "            q = np.zeros(l + d + 1)\n",
      "            q[d + 1:] = self.C / l\n",
      "            G = np.zeros((2 * l, l + d + 1))\n",
      "            G[:l, :d] = X\n",
      "            G[:l, d] = 1\n",
      "            G[:l] = G[:l] * y[:, np.newaxis]\n",
      "            G[:l, d + 1:][range(l), range(l)] = 1\n",
      "            G[l:, d + 1:][range(l), range(l)] = 1\n",
      "            h = np.zeros(2 * l)\n",
      "            h[:l] = 1\n",
      "            G = -G\n",
      "            h = -h\n",
      "            P = matrix(P)\n",
      "            q = matrix(q)\n",
      "            G = matrix(G)\n",
      "            h = matrix(h)\n",
      "            w = np.array(solvers.qp(P, q, G, h)['x']).reshape(l + d + 1)\n",
      "            self.w = w[:d]\n",
      "            self.w0 = w[d]\n",
      "        else:\n",
      "            q = np.ones(l)\n",
      "            G = np.zeros((2 * l, l))\n",
      "            G[range(l), range(l)] = 1\n",
      "            G[range(l, 2 * l), range(l)] = -1\n",
      "            h = np.zeros(2 * l)\n",
      "            h[:l] = self.C / l\n",
      "            A = y[np.newaxis, :]\n",
      "            b = np.zeros(1)\n",
      "            if self.kernel == 'linear':\n",
      "                P = np.dot(X, X.T) * y[:, np.newaxis] * y[np.newaxis, :]\n",
      "            elif self.kernel == 'polynomial':\n",
      "                self.X = X\n",
      "                self.y = y\n",
      "                P = ((np.dot(X, X.T) + 1) ** self.degree) * y[:, np.newaxis] * y[np.newaxis, :]\n",
      "            else:\n",
      "                self.X = X\n",
      "                self.y = y\n",
      "                norm2 = normed_sq(X, X)\n",
      "                K = np.exp(-self.gamma * norm2)\n",
      "                P = K * y[:, np.newaxis] * y[np.newaxis, :]\n",
      "            q = -q\n",
      "            P = P.astype(float)\n",
      "            P = matrix(P)\n",
      "            q = matrix(q)\n",
      "            G = matrix(G)\n",
      "            h = matrix(h)\n",
      "            A = A.astype(float)\n",
      "            A = matrix(A)\n",
      "            b = matrix(b)\n",
      "            self.dual = np.array(solvers.qp(P, q, G, h, A, b)['x']).reshape(l)\n",
      "            if self.kernel == 'linear':\n",
      "                self.w = (X * y[:, np.newaxis] * self.dual[:, np.newaxis]).sum(axis=0)\n",
      "                idx = np.where(np.all([~np.isclose(self.dual, 0, atol=1e-11), self.dual < self.C/l], axis=0))[0]\n",
      "                self.w0 = (- np.dot(self.w, X[idx].T).sum() + y[idx].sum()) / len(idx)\n",
      "            \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Метод для получения предсказаний на данных\n",
      "        \n",
      "        X - переменная типа numpy.array, признаковые описания объектов из обучающей выборки\n",
      "        \"\"\"\n",
      "        if self.method == 'primal' or self.kernel == 'linear':\n",
      "            predict = np.dot(X, self.w) + self.w0\n",
      "        elif self.kernel == 'rbf':\n",
      "            l = self.X.shape[0]\n",
      "            idx = np.where(np.all([~np.isclose(self.dual, 0, atol=1e-11), self.dual < self.C/l], axis=0))[0]\n",
      "            norm1_2 = normed_sq(self.X, X)\n",
      "            K = np.exp(-self.gamma * norm1_2)\n",
      "            norm2_2 = normed_sq(self.X, self.X[idx])\n",
      "            K2 = np.exp(-self.gamma * norm2_2)\n",
      "            sum1 = (K * self.y[:, np.newaxis] * self.dual[:, np.newaxis]).sum(axis=0)\n",
      "            sum2 = (K2 * self.y[:, np.newaxis] * self.dual[:, np.newaxis]).sum()\n",
      "            predict = sum1 + ((np.sum(self.y[idx]) - sum2) / len(idx))\n",
      "        elif self.kernel == 'polynomial':\n",
      "            l = self.X.shape[0]\n",
      "            idx = np.where(np.all([~np.isclose(self.dual, 0, atol=1e-11), self.dual < self.C/l], axis=0))[0]\n",
      "            K = (np.dot(self.X, X.T) + 1) ** self.degree\n",
      "            K2 = (np.dot(self.X, self.X[idx].T) + 1) ** self.degree\n",
      "            sum1 = (K * self.y[:, np.newaxis] * self.dual[:, np.newaxis]).sum(axis=0)\n",
      "            sum2 = (K2 * self.y[:, np.newaxis] * self.dual[:, np.newaxis]).sum()\n",
      "            predict = sum1 + ((np.sum(self.y[idx]) - sum2) / len(idx))\n",
      "        predict[predict > 0] = 1\n",
      "        predict[predict <= 0] = -1\n",
      "        return predict\n",
      "        \n",
      "    def get_w(self, X=None, y=None):\n",
      "        \"\"\"\n",
      "        Получить прямые переменные (без учёта w_0)\n",
      "        \n",
      "        Если method = 'dual', а ядро линейное, переменные должны быть получены\n",
      "        с помощью выборки (X, y) \n",
      "        \n",
      "        return: одномерный numpy array\n",
      "        \"\"\"\n",
      "        return self.w\n",
      "\n",
      "    def get_w0(self, X=None, y=None):\n",
      "        \"\"\"\n",
      "        Получить вектор сдвига\n",
      "        \n",
      "        Если method = 'dual', а ядро линейное, переменные должны быть получены\n",
      "        с помощью выборки (X, y) \n",
      "        \n",
      "        return: float\n",
      "        \"\"\"\n",
      "        return self.w0\n",
      "        \n",
      "    def get_dual(self):\n",
      "        \"\"\"\n",
      "        Получить двойственные переменные\n",
      "        \n",
      "        return: одномерный numpy array\n",
      "        \"\"\"\n",
      "        return self.dual\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class RleSequence:\n",
      "    def __init__(self, input_sequence):\n",
      "        def encode_rle(X):\n",
      "            idx = np.where(np.append([np.nan], np.diff(X)) != 0)\n",
      "            amount = np.diff(np.append(idx, len(X)))\n",
      "            return X[idx], amount\n",
      "        \n",
      "        self.elems, self.amount = encode_rle(input_sequence)\n",
      "        self.len = len(input_sequence)\n",
      "        \n",
      "    def __getitem__(self, i):\n",
      "        if isinstance(i, int):\n",
      "            if i < 0:\n",
      "                i = self.len + i\n",
      "            idx = np.where(i <= np.cumsum(self.amount) - 1)[0][0]\n",
      "            return self.elems[idx]\n",
      "        if isinstance(i, slice):\n",
      "            arr = []\n",
      "            for idx in range(*i.indices(self.len)):\n",
      "                if idx < 0:\n",
      "                    idx = self.len + idx\n",
      "                idx2 = np.where(idx <= np.cumsum(self.amount) - 1)[0][0]\n",
      "                arr.append(self.elems[idx2])\n",
      "            return np.array(arr)\n",
      "        raise IndexError\n",
      "        \n",
      "    def __contains__(self, elem):\n",
      "        return (elem in self.elems)\n",
      "    \n",
      "    def __iter__(self):\n",
      "        self.curr_elem_idx = 0\n",
      "        self.curr_len = 0\n",
      "        self.curr_cumsum = self.amount[0]\n",
      "        return self\n",
      "    \n",
      "    def __next__(self):\n",
      "        if self.curr_len >= self.len:\n",
      "            raise StopIteration\n",
      "        if self.curr_len == self.curr_cumsum:\n",
      "            self.curr_elem_idx += 1\n",
      "            self.curr_cumsum += self.amount[self.curr_elem_idx]\n",
      "        self.curr_len += 1\n",
      "        return self.elems[self.curr_elem_idx]\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "class MulticlassStrategy:   \n",
      "    def __init__(self, classifier, mode, **kwargs):\n",
      "        \"\"\"\n",
      "        Инициализация мультиклассового классификатора\n",
      "        \n",
      "        classifier - базовый бинарный классификатор\n",
      "        \n",
      "        mode - способ решения многоклассовой задачи,\n",
      "        либо 'one_vs_all', либо 'all_vs_all'\n",
      "        \n",
      "        **kwargs - параметры классификатор\n",
      "        \"\"\"\n",
      "        self.classifier = classifier\n",
      "        self.mode = mode\n",
      "        self.kwargs = kwargs\n",
      "        \n",
      "    def fit(self, X, y):\n",
      "        \"\"\"\n",
      "        Обучение классификатора\n",
      "        \"\"\"\n",
      "        self.classifiers = []\n",
      "        if self.mode == 'one_vs_all':\n",
      "            for j in range(len(np.unique(y))):\n",
      "                self.classifiers.append(self.classifier(**self.kwargs))\n",
      "                new_y = np.ones(len(y))\n",
      "                new_y[y != j + 1] = -1\n",
      "                new_y = new_y.astype(int)\n",
      "                self.classifiers[j].fit(X, new_y)\n",
      "        else:\n",
      "            for i in range(len(np.unique(y)) - 1):\n",
      "                for j in range(i + 1, len(np.unique(y))):\n",
      "                    clf = self.classifier(**self.kwargs)\n",
      "                    mask1 = y == i + 1\n",
      "                    mask2 = y == j + 1\n",
      "                    mask = np.any([mask1, mask2], axis=0)\n",
      "                    new_X = X[mask]\n",
      "                    new_y = y.copy()\n",
      "                    new_y[mask1] = -1\n",
      "                    new_y[mask2] = 1\n",
      "                    new_y = new_y[mask]\n",
      "                    clf.fit(new_X, new_y)\n",
      "                    self.classifiers.append(clf)\n",
      "        \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Выдача предсказаний классификатором\n",
      "        \"\"\"\n",
      "        if self.mode == 'one_vs_all':\n",
      "            probability_matrix = np.empty([len(self.classifiers), X.shape[0]])\n",
      "            for idx, classifier in enumerate(self.classifiers):\n",
      "                probability_vec = classifier.predict_proba(X).T[1]\n",
      "                probability_matrix[idx] = probability_vec\n",
      "            prediction_vec = probability_matrix.argmax(axis=0) + 1\n",
      "            return prediction_vec\n",
      "        else:\n",
      "            k = 0\n",
      "            prediction_matrix = np.zeros([X.shape[0], len(np.unique(y))])\n",
      "            for i in range(len(np.unique(y)) - 1):\n",
      "                for j in range(i + 1, len(np.unique(y))):\n",
      "                    prediction = self.classifiers[k].predict(X)\n",
      "                    prediction_matrix[prediction == -1, i] += 1\n",
      "                    prediction_matrix[prediction == +1, j] += 1\n",
      "                    k += 1\n",
      "            prediction = prediction_matrix.argmax(axis=1) + 1\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import scipy\n",
      "\n",
      "\n",
      "from scipy.misc import logsumexp\n",
      "from scipy.sparse import csr_matrix\n",
      "from scipy.special import expit\n",
      "\n",
      "\n",
      "class BaseSmoothOracle:\n",
      "    \"\"\"\n",
      "    Базовый класс для реализации оракулов.\n",
      "    \"\"\"\n",
      "    def func(self, w):\n",
      "        \"\"\"\n",
      "        Вычислить значение функции в точке w.\n",
      "        \"\"\"\n",
      "        k = self.X.shape[0]\n",
      "        if len(w.shape) == 1:\n",
      "            log = np.logaddexp(0, -self.y * self.X.dot(w))\n",
      "            loss = log.sum(axis=0) / k + (self.l2_coef * np.dot(w, w) / 2)\n",
      "        else:\n",
      "            alphas = self.X.dot(w.T) - np.amax(self.X.dot(w.T), axis=1)[:, np.newaxis]\n",
      "            softmax = np.exp(alphas) / np.sum(np.exp(alphas), axis=1)[:, np.newaxis]\n",
      "            loss = -np.log(softmax[range(k), self.y]).sum() / k + (self.l2_coef * np.sum(np.diag(w.dot(w.T))) / 2)\n",
      "        return loss\n",
      "\n",
      "    def grad(self, w):\n",
      "        \"\"\"\n",
      "        Вычислить значение градиента функции в точке w.\n",
      "        \"\"\"\n",
      "        k = self.X.shape[0]\n",
      "        if len(w.shape) == 1:\n",
      "            if isinstance(self.X, csr_matrix):\n",
      "                vec = - self.X.multiply(self.y[:, np.newaxis]).multiply(expit(- self.y * self.X.dot(w))[:, np.newaxis])\n",
      "                grad = np.array(vec.sum(axis=0))[0] / k + self.l2_coef * w\n",
      "            else:\n",
      "                vec = - self.y[:, np.newaxis] * (self.X * expit(- self.y * self.X.dot(w))[:, np.newaxis])\n",
      "                grad = vec.sum(axis=0) / k + self.l2_coef * w\n",
      "        else:\n",
      "            alphas = self.X.dot(w.T) - np.amax(self.X.dot(w.T), axis=1)[:, np.newaxis]\n",
      "            softmax = np.exp(alphas) / np.sum(np.exp(alphas), axis=1)[:, np.newaxis]\n",
      "            mask = np.zeros((self.class_number, k), dtype=bool)\n",
      "            mask[np.unique(self.y)] = self.y == np.unique(self.y)[:, np.newaxis]\n",
      "            if isinstance(self.X, csr_matrix):\n",
      "                c1 = (self.X.T * softmax).T\n",
      "                c = (c1 - self.X.T.dot(mask.T).T) / k\n",
      "            else:\n",
      "                c = ((self.X[:, :, np.newaxis] * softmax[:, np.newaxis, :]).sum(axis=0).T - self.X.T.dot(mask.T).T) / k\n",
      "            grad = c + self.l2_coef * w\n",
      "        return grad\n",
      "\n",
      "        \n",
      "class BinaryLogistic(BaseSmoothOracle):\n",
      "    \"\"\"\n",
      "    Оракул для задачи двухклассовой логистической регрессии.\n",
      "    \n",
      "    Оракул должен поддерживать l2 регуляризацию.\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, l2_coef):\n",
      "        \"\"\"\n",
      "        Задание параметров оракула.\n",
      "        \n",
      "        l2_coef - коэффициент l2 регуляризации\n",
      "        \"\"\"\n",
      "        self.l2_coef = l2_coef\n",
      "     \n",
      "    def func(self, X, y, w):\n",
      "        \"\"\"\n",
      "        Вычислить значение функционала в точке w на выборке X с ответами y.\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        w - одномерный numpy array\n",
      "        \"\"\"\n",
      "        \n",
      "        self.X = X\n",
      "        self.y = y\n",
      "        return super().func(w)\n",
      "        \n",
      "    def grad(self, X, y, w):\n",
      "        \"\"\"\n",
      "        Вычислить градиент функционала в точке w на выборке X с ответами y.\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        w - одномерный numpy array\n",
      "        \"\"\"\n",
      "        self.X = X\n",
      "        self.y = y\n",
      "        return super().grad(w)\n",
      "    \n",
      "    \n",
      "class MulticlassLogistic(BaseSmoothOracle):\n",
      "    \"\"\"\n",
      "    Оракул для задачи многоклассовой логистической регрессии.\n",
      "    \n",
      "    Оракул должен поддерживать l2 регуляризацию.\n",
      "    \n",
      "    w в этом случае двумерный numpy array размера (class_number, d),\n",
      "    где class_number - количество классов в задаче, d - размерность задачи\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, class_number=3, l2_coef=1):\n",
      "        \"\"\"\n",
      "        Задание параметров оракула.\n",
      "        \n",
      "        class_number - количество классов в задаче\n",
      "        \n",
      "        l2_coef - коэффициент l2 регуляризации\n",
      "        \"\"\"\n",
      "        self.class_number = class_number\n",
      "        self.l2_coef = l2_coef\n",
      "     \n",
      "    def func(self, X, y, w):\n",
      "        \"\"\"\n",
      "        Вычислить значение функционала в точке w на выборке X с ответами y.\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        w - одномерный numpy array\n",
      "        \"\"\"\n",
      "        self.X = X\n",
      "        self.y = y\n",
      "        return super().func(w)\n",
      "        \n",
      "    def grad(self, X, y, w):\n",
      "        \"\"\"\n",
      "        Вычислить значение функционала в точке w на выборке X с ответами y.\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        w - одномерный numpy array\n",
      "        \"\"\"\n",
      "        self.X = X\n",
      "        self.y = y\n",
      "        return super().grad(w)\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def replace_nan_to_means(X):\n",
      "    X_copy = np.copy(X)\n",
      "    mask = np.isnan(X_copy)\n",
      "    not_mask = ~mask\n",
      "    all_is_nan = ~not_mask.any(axis=0)\n",
      "    X_copy.T[all_is_nan] = 0\n",
      "    mean_arr = np.nanmean(X_copy, axis=0)\n",
      "    mean_matrix = np.vstack((mean_arr, ) * X.shape[0])\n",
      "    mask = np.isnan(X_copy)\n",
      "    X_copy[mask] = mean_matrix[mask]\n",
      "    return X_copy\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import random\n",
      "import numpy as np\n",
      "from nearest_neighbors import KNNClassifier\n",
      "\n",
      "\n",
      "eps = 10 ** -5\n",
      "\n",
      "\n",
      "def accuracy(y_real, y_predict):\n",
      "    y = np.isclose(y_real, y_predict)\n",
      "    return y.cumsum()[-1] / len(y)\n",
      "\n",
      "\n",
      "def kfold(n, n_folds):\n",
      "    each_volume = n // n_folds\n",
      "    folds = np.array([each_volume] * n_folds)\n",
      "    k = n - each_volume * n_folds\n",
      "    folds[:k] += 1\n",
      "    indexes_full = set(range(n))\n",
      "    indexes_remainder = set(range(n))\n",
      "    validation = []\n",
      "    train = []\n",
      "    for i in range(n_folds):\n",
      "        curr_validation = random.sample(indexes_remainder, folds[i])\n",
      "        curr_train = list(indexes_full - set(curr_validation))\n",
      "        indexes_remainder -= set(curr_validation)\n",
      "        validation.append(np.array(curr_validation))\n",
      "        train.append(np.array(curr_train))\n",
      "    return list(zip(train, validation))\n",
      " \n",
      "\n",
      "def new_predict(k, k_prev, kneighbors, X_test, y_train, weights, classes, dic):\n",
      "    prediction = []\n",
      "    if weights:\n",
      "        distance = kneighbors[0]\n",
      "        idx = kneighbors[1]\n",
      "        for i in range(X_test.shape[0]):\n",
      "            for j in range(k_prev, k):\n",
      "                dic[i][y_train[idx[i][j]]] += 1 / (distance[i][j] + eps)\n",
      "            prediction.append(sorted(dic[i].items(), key=lambda x: - x[1])[0][0])\n",
      "    else:\n",
      "        idx = kneighbors\n",
      "        for i in range(X_test.shape[0]):\n",
      "            for j in range(k_prev, k):\n",
      "                dic[i][y_train[idx[i][j]]] += 1\n",
      "            prediction.append(sorted(dic[i].items(), key=lambda x: - x[1])[0][0])\n",
      "    return np.array(prediction)\n",
      "\n",
      "\n",
      "def knn_cross_val_score(X, y, k_list, score, cv=None, **kwargs):\n",
      "    max_k = k_list[-1]\n",
      "    classes = []\n",
      "    if cv is None:\n",
      "        cv = kfold(len(y), 3)\n",
      "    kwargs['k'] = max_k\n",
      "    clf = KNNClassifier(**kwargs)\n",
      "    y_train_all = []\n",
      "    y_test_all = []\n",
      "    X_test_all = []\n",
      "    kneighbors_all = []\n",
      "    for tup in cv:\n",
      "        X_train = X[tup[0], :]\n",
      "        y_train = y[tup[0]]\n",
      "        y_train_all.append(y_train)\n",
      "        X_test = X[tup[1], :]\n",
      "        X_test_all.append(X_test)\n",
      "        y_test = y[tup[1]]\n",
      "        y_test_all.append(y_test)\n",
      "        clf.fit(X_train, y_train)\n",
      "        kneighbors = clf.find_kneighbors(X_test, kwargs['weights'])\n",
      "        kneighbors_all.append(kneighbors)\n",
      "        classes.append(np.unique(y_train))\n",
      "    dic = {}\n",
      "    dic_neigh_list = []\n",
      "    for i in range(len(cv)):\n",
      "        l = []\n",
      "        for j in range(len(X_test_all[i])):\n",
      "            l.append(dict(zip(classes[i], np.zeros(len(classes[i])))))\n",
      "        dic_neigh_list.append(l)\n",
      "    k_prev = 0\n",
      "    for k in k_list:\n",
      "        acc = np.empty(len(cv))\n",
      "        for i in range(len(cv)):\n",
      "            prediction = new_predict(k, k_prev, kneighbors_all[i], X_test_all[i], y_train_all[i], kwargs['weights'], \n",
      "                                     classes[i], dic_neigh_list[i])\n",
      "            acc[i] = accuracy(y_test_all[i], prediction)\n",
      "        dic[k] = acc\n",
      "        k_prev = k\n",
      "    return dic\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "def encode_rle(x):\n",
      "    x_shift_r = np.append([np.nan], x[:-1])\n",
      "    previous_the_same_mask = x == x_shift_r\n",
      "    numbers = x[~previous_the_same_mask]\n",
      "    cumsum = np.cumsum(previous_the_same_mask)\n",
      "    csum_shift_l = np.append(cumsum[1:], [np.nan])\n",
      "    next_the_same_mask = cumsum == csum_shift_l\n",
      "    next_the_same_mask[-1] = True\n",
      "    full_cumsum = cumsum[next_the_same_mask] + range(1, len(cumsum[next_the_same_mask]) + 1)\n",
      "    shifted_full_cumsum = np.append([0], full_cumsum[:-1])\n",
      "    amount = full_cumsum - shifted_full_cumsum\n",
      "    return numbers, amount\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "import oracles\n",
      "import scipy\n",
      "import time\n",
      "\n",
      "\n",
      "from numpy.linalg import norm\n",
      "from scipy.sparse import csr_matrix\n",
      "from scipy.special import expit\n",
      "\n",
      "\n",
      "class GDClassifier:\n",
      "    \"\"\"\n",
      "    Реализация метода градиентного спуска для произвольного\n",
      "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
      "    \"\"\"\n",
      "    def __init__(self, loss_function='binary_logistic', step_alpha=1, step_beta=0, \n",
      "                 tolerance=1e-5, max_iter=1000, **kwargs):\n",
      "        \"\"\"\n",
      "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
      "        Может принимать значения:\n",
      "        - 'binary_logistic' - бинарная логистическая регрессия\n",
      "        - 'multinomial_logistic' - многоклассовая логистическая регрессия\n",
      "                \n",
      "        step_alpha - float, параметр выбора шага из текста задания\n",
      "        \n",
      "        step_beta- float, параметр выбора шага из текста задания\n",
      "        \n",
      "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию.\n",
      "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
      "        если (f(x_{k+1}) - f(x_{k})) < tolerance: то выход \n",
      "        \n",
      "        max_iter - максимальное число итераций     \n",
      "        \n",
      "        **kwargs - аргументы, необходимые для инициализации   \n",
      "        \"\"\"\n",
      "        self.loss_function = loss_function\n",
      "        self.step_alpha = step_alpha\n",
      "        self.step_beta = step_beta\n",
      "        self.tolerance = tolerance\n",
      "        self.max_iter = max_iter\n",
      "        self.kwargs = kwargs\n",
      "          \n",
      "    def fit(self, X, y, X_test=None, w_0=None, trace=False):\n",
      "        \"\"\"\n",
      "        Обучение метода по выборке X с ответами y\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        w_0 - начальное приближение в методе\n",
      "        \n",
      "        trace - переменная типа bool\n",
      "      \n",
      "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
      "        о поведении метода. Длина словаря history = количество итераций + 1 (начальное приближение)\n",
      "        \n",
      "        history['time']: list of floats, содержит интервалы времени между двумя итерациями метода\n",
      "        history['func']: list of floats, содержит значения функции на каждой итерации\n",
      "        (0 для самой первой точки)\n",
      "        \"\"\"\n",
      "        if self.loss_function == 'binary_logistic':\n",
      "            self.oracle = oracles.BinaryLogistic(self.kwargs['l2_coef'])\n",
      "        else:\n",
      "            if 'class_number' in self.kwargs.keys():\n",
      "                self.oracle = oracles.MulticlassLogistic(self.kwargs['class_number'], self.kwargs['l2_coef'])\n",
      "            else:\n",
      "                self.oracle = oracles.MulticlassLogistic(l2_coef=self.kwargs['l2_coef'])\n",
      "        if w_0 is None:\n",
      "            if self.loss_function == 'binary_logistic':\n",
      "                w_0 = np.zeros(X.shape[1])\n",
      "            else:\n",
      "                w_0 = np.zeros((self.kwargs['class_number'], X.shape[1]))\n",
      "        self.w = w_0\n",
      "        w_prev = w_0\n",
      "        if X_test is not None:\n",
      "            self.pred = [self.predict(X_test)]\n",
      "        history = {'time': [0], 'func': [self.get_objective(X, y)]}\n",
      "        for i in range(self.max_iter):\n",
      "            start = time.time()\n",
      "            self.w = w_prev - (self.step_alpha / (i + 1) ** self.step_beta) * self.get_gradient(X, y)\n",
      "            history['func'].append(self.get_objective(X, y))\n",
      "            if X_test is not None:\n",
      "                self.pred.append(self.predict(X_test))\n",
      "            difference = np.absolute(history['func'][-1] - history['func'][-2])\n",
      "            w_prev = self.w\n",
      "            history['time'].append(time.time() - start)\n",
      "            if difference < self.tolerance:\n",
      "                break\n",
      "        if trace:\n",
      "            return history\n",
      "        \n",
      "    def predict(self, X):\n",
      "        \"\"\"\n",
      "        Получение меток ответов на выборке X\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        return: одномерный numpy array с предсказаниями\n",
      "        \"\"\"\n",
      "        pred = np.argmax(self.predict_proba(X), axis=1)\n",
      "        if self.loss_function == 'binary_logistic':\n",
      "            pred[pred == 0] = -1\n",
      "        return pred\n",
      "\n",
      "    def predict_proba(self, X):\n",
      "        \"\"\"\n",
      "        Получение вероятностей принадлежности X к классу k\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        return: двумерной numpy array, [i, k] значение соответветствует вероятности\n",
      "        принадлежности i-го объекта к классу k \n",
      "        \"\"\"\n",
      "        if self.loss_function == 'binary_logistic':\n",
      "            probability = expit(X.dot(self.w))\n",
      "            predict_proba = np.vstack(((1 - probability), probability)).T\n",
      "        else:\n",
      "            alphas = X.dot(self.w.T) - np.amax(X.dot(self.w.T), axis=1)[:, np.newaxis]\n",
      "            predict_proba = np.exp(alphas) / np.sum(np.exp(alphas), axis=1)[:, np.newaxis]\n",
      "        return predict_proba\n",
      "            \n",
      "    def get_objective(self, X, y):\n",
      "        \"\"\"\n",
      "        Получение значения целевой функции на выборке X с ответами y\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        return: float\n",
      "        \"\"\"\n",
      "        return self.oracle.func(X, y, self.w)\n",
      "        \n",
      "    def get_gradient(self, X, y):\n",
      "        \"\"\"\n",
      "        Получение значения градиента функции на выборке X с ответами y\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        y - одномерный numpy array\n",
      "        \n",
      "        return: numpy array, размерность зависит от задачи\n",
      "        \"\"\"\n",
      "        return self.oracle.grad(X, y, self.w)\n",
      "    \n",
      "    def get_weights(self):\n",
      "        \"\"\"\n",
      "        Получение значения весов функционала\n",
      "        \"\"\"\n",
      "        return self.w\n",
      "        \n",
      "\n",
      "class SGDClassifier(GDClassifier):\n",
      "    \"\"\"\n",
      "    Реализация метода стохастического градиентного спуска для произвольного\n",
      "    оракула, соответствующего спецификации оракулов из модуля oracles.py\n",
      "    \"\"\"\n",
      "    \n",
      "    def __init__(self, loss_function='binary_logistic', batch_size=1, step_alpha=1, step_beta=0, \n",
      "                 tolerance=1e-5, max_iter=1000, random_seed=153, **kwargs):\n",
      "        \"\"\"\n",
      "        loss_function - строка, отвечающая за функцию потерь классификатора. \n",
      "        Может принимать значения:\n",
      "        - 'binary_logistic' - бинарная логистическая регрессия\n",
      "        - 'multinomial_logistic' - многоклассовая логистическая регрессия\n",
      "        \n",
      "        batch_size - размер подвыборки, по которой считается градиент\n",
      "        \n",
      "        step_alpha - float, параметр выбора шага из текста задания\n",
      "        \n",
      "        step_beta- float, параметр выбора шага из текста задания\n",
      "        \n",
      "        tolerance - точность, по достижении которой, необходимо прекратить оптимизацию\n",
      "        Необходимо использовать критерий выхода по модулю разности соседних значений функции:\n",
      "        если (f(x_{k+1}) - f(x_{k})) < tolerance: то выход \n",
      "        \n",
      "        \n",
      "        max_iter - максимальное число итераций\n",
      "        \n",
      "        random_seed - в начале метода fit необходимо вызвать np.random.seed(random_seed).\n",
      "        Этот параметр нужен для воспроизводимости результатов на разных машинах.\n",
      "        \n",
      "        **kwargs - аргументы, необходимые для инициализации\n",
      "        \"\"\"\n",
      "        self.loss_function = loss_function\n",
      "        self.batch_size = batch_size\n",
      "        self.step_alpha = step_alpha\n",
      "        self.step_beta = step_beta\n",
      "        self.tolerance = tolerance\n",
      "        self.max_iter = max_iter\n",
      "        self.random_seed = random_seed\n",
      "        self.kwargs = kwargs\n",
      "        \n",
      "    def fit(self, X, y, X_test=None, w_0=None, trace=False, log_freq=1):\n",
      "        \"\"\"\n",
      "        Обучение метода по выборке X с ответами y\n",
      "        \n",
      "        X - scipy.sparse.csr_matrix или двумерный numpy.array\n",
      "        \n",
      "        y - одномерный numpy array\n",
      "                \n",
      "        w_0 - начальное приближение в методе\n",
      "        \n",
      "        Если trace = True, то метод должен вернуть словарь history, содержащий информацию \n",
      "        о поведении метода. Если обновлять history после каждой итерации, метод перестанет \n",
      "        превосходить в скорости метод GD. Поэтому, необходимо обновлять историю метода лишь\n",
      "        после некоторого числа обработанных объектов в зависимости от приближённого номера эпохи.\n",
      "        Приближённый номер эпохи:\n",
      "            {количество объектов, обработанных методом SGD} / {количество объектов в выборке}\n",
      "        \n",
      "        log_freq - float от 0 до 1, параметр, отвечающий за частоту обновления. \n",
      "        Обновление должно проиходить каждый раз, когда разница между двумя значениями приближённого номера эпохи\n",
      "        будет превосходить log_freq.\n",
      "        \n",
      "        history['epoch_num']: list of floats, в каждом элементе списка будет записан приближённый номер эпохи:\n",
      "        history['time']: list of floats, содержит интервалы времени между двумя соседними замерами\n",
      "        history['func']: list of floats, содержит значения функции после текущего приближённого номера эпохи\n",
      "        \"\"\"\n",
      "        np.random.seed(self.random_seed)\n",
      "        k = X.shape[0]\n",
      "        processed = 0\n",
      "        epoch_num = 0\n",
      "        if self.loss_function == 'binary_logistic':\n",
      "            self.oracle = oracles.BinaryLogistic(self.kwargs['l2_coef'])\n",
      "        else:\n",
      "            if 'class_number' in self.kwargs.keys():\n",
      "                self.oracle = oracles.MulticlassLogistic(self.kwargs['class_number'], self.kwargs['l2_coef'])\n",
      "            else:\n",
      "                self.oracle = oracles.MulticlassLogistic(l2_coef=self.kwargs['l2_coef'])\n",
      "        if w_0 is None:\n",
      "            if self.loss_function == 'binary_logistic':\n",
      "                w_0 = np.zeros(X.shape[1])\n",
      "            else:\n",
      "                w_0 = np.zeros((self.kwargs['class_number'], X.shape[1]))\n",
      "        self.w = w_0\n",
      "        w_prev = w_0\n",
      "        history = {'time': [0], 'func': [self.get_objective(X, y)], 'epoch_num': [0]}\n",
      "        start = time.time()\n",
      "        w_epoch = w_0\n",
      "        if X_test is not None:\n",
      "            self.pred = [self.predict(X_test)]\n",
      "        for i in range(self.max_iter):\n",
      "            idx = np.random.permutation(k)\n",
      "            elems = X[idx[:self.batch_size]]\n",
      "            step = self.step_alpha / (i + 1) ** self.step_beta\n",
      "            self.w = w_prev - step * self.get_gradient(elems, y[idx[:self.batch_size]])\n",
      "            processed += self.batch_size\n",
      "            if processed / k - epoch_num > log_freq:\n",
      "                if X_test is not None:\n",
      "                    self.pred.append(self.predict(X_test))\n",
      "                history['time'].append(time.time() - start)\n",
      "                start = time.time()\n",
      "                history['func'].append(self.get_objective(X, y))\n",
      "                w_epoch = self.w\n",
      "                epoch_num = processed / k\n",
      "                history['epoch_num'].append(epoch_num)\n",
      "                difference = np.absolute(history['func'][-1] - history['func'][-2])\n",
      "                if difference < self.tolerance:\n",
      "                    break\n",
      "            w_prev = self.w\n",
      "        if trace:\n",
      "            return history\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "def find_word_in_circle(circle, word):\n",
      "    if len(word) > len(circle):\n",
      "        return 'NO'\n",
      "    else:\n",
      "        double_circle = circle * 2\n",
      "        if word in double_circle or word[::-1] in double_circle:\n",
      "            return 'YES'\n",
      "        else:\n",
      "            return 'NO'\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "def subreverse_sequence(sequence):\n",
      "    new_seq = sequence[::2]\n",
      "    new_seq = new_seq[::-1] + sequence[1::2]\n",
      "    return tuple(new_seq)\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import functools\n",
      "\n",
      "\n",
      "def with_arguments(deco_with_args):\n",
      "    @functools.wraps(deco_with_args)\n",
      "    def deco_without_args(*args, **kwargs):\n",
      "        def wrapper(f):\n",
      "            result = deco_with_args(f, *args, **kwargs)\n",
      "            functools.update_wrapper(result, f)\n",
      "            return result\n",
      "        return wrapper\n",
      "    return deco_without_args\n",
      "\n",
      "\n",
      "@with_arguments\n",
      "def check_arguments(f, *args):\n",
      "    types = args\n",
      "\n",
      "    def wrapper(*args, **kwargs):\n",
      "        if len(args) < len(types):\n",
      "            raise TypeError\n",
      "        for idx, tp in enumerate(types):\n",
      "            if not isinstance(args[idx], tp):\n",
      "                raise TypeError\n",
      "        return f(*args, **kwargs)\n",
      "    return wrapper\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class CooSparseMatrix:\n",
      "    def __init__(self, dense_matrix, shape=None):\n",
      "        if shape is None:\n",
      "            self.shape = dense_matrix.shape\n",
      "        elif shape >= dense_matrix.shape:\n",
      "            self.shape = shape\n",
      "        else:\n",
      "            raise TypeError\n",
      "        self.matrix = {}\n",
      "        for idx1, row in enumerate(dense_matrix):\n",
      "            for idx2, item in enumerate(row):\n",
      "                if not np.isclose(item, 0):\n",
      "                    self.matrix[idx1, idx2] = item\n",
      "    \n",
      "    def add_element(self, value, coords):\n",
      "        if coords < self.shape and coords not in self.matrix.keys():\n",
      "            if not np.isclose(value, 0):\n",
      "                self.matrix[coords] = value\n",
      "        else:\n",
      "            raise TypeError\n",
      "        \n",
      "    def __add__(self, coo_matrix):\n",
      "        if self.shape == coo_matrix.shape:\n",
      "            tmp_matrix = CooSparseMatrix(np.array([[0]]), self.shape)\n",
      "            both_coords = set(self.matrix.keys()).union(set(coo_matrix.matrix.keys()))\n",
      "            for coords in both_coords:\n",
      "                item1 = self.matrix[coords] if coords in self.matrix.keys() else 0\n",
      "                item2 = coo_matrix.matrix[coords] if coords in coo_matrix.matrix.keys() else 0\n",
      "                tmp_matrix.add_element(item1 + item2, coords)\n",
      "        else:\n",
      "            raise TypeError\n",
      "        return tmp_matrix\n",
      "    \n",
      "    def __mul__(self, value):\n",
      "        tmp_matrix = CooSparseMatrix(np.array([[0]]), self.shape)\n",
      "        for coords in self.matrix.keys():\n",
      "            tmp_matrix.add_element(value * self.matrix[coords], coords)\n",
      "        return tmp_matrix\n",
      "    \n",
      "    def __getitem__(self, i):\n",
      "        if i < self.shape[0]:\n",
      "            row_i = np.array([])\n",
      "            for j in range(self.shape[1]):\n",
      "                if (i, j) in self.matrix.keys():\n",
      "                    row_i = np.append(row_i, self.matrix[i, j])\n",
      "                else:\n",
      "                    row_i = np.append(row_i, 0)\n",
      "        else:\n",
      "            raise TypeError\n",
      "        return row_i\n",
      "    \n",
      "    def toarray(self):\n",
      "        arr = self[0]\n",
      "        for i in range(1, self.shape[0]):\n",
      "            arr = np.vstack((arr, self[i]))\n",
      "        return arr\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "class MulticlassStrategy:\n",
      "    def __init__(self, classifier, **kwargs):\n",
      "        self.bin_classifier = classifier\n",
      "        self.bin_args = kwargs\n",
      "    \n",
      "    def fit(self, X, y):\n",
      "        classes = np.unique(y)\n",
      "        self.classifiers = np.array([])\n",
      "        for j in sorted(classes):\n",
      "            self.classifiers = np.append(self.classifiers, self.bin_classifier(**self.bin_args))\n",
      "            new_y = np.ones(len(y))\n",
      "            for i in range(len(y)):\n",
      "                if y[i] != j:\n",
      "                    new_y[i] = -1\n",
      "            new_y = new_y.astype(int)\n",
      "            self.classifiers[j].fit(X, new_y)\n",
      "    \n",
      "    def predict(self, X):\n",
      "        probability_matrix = np.empty([len(self.classifiers), X.shape[0]])\n",
      "        for idx, classifier in enumerate(self.classifiers):\n",
      "            probability_vec = classifier.predict_proba(X).T[1]\n",
      "            probability_matrix[idx] = probability_vec\n",
      "        prediction_vec = probability_matrix.argmax(axis=0)\n",
      "        return prediction_vec\n",
      "\n",
      "\n",
      "\n",
      "=========================================\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for code in data['Dragunov_Nikita'].values():\n",
    "    print(code)\n",
    "    print(\"\\n\\n=========================================\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Dragunov_Nikita', 'Dzhenakov_Dmitriy', 'Filimonov_Vladislav', 'Fominskaya_Galina', 'Iavorskaia_Maria', 'Medvedev_Alex', 'Medvedev_Dmitry', 'Poletaev_Vsevolod', 'Ponomareva_Lubov', 'Rudnev_Viktor', 'Sagaydak_Oleg', 'Shamshiev_Mamat', 'Shatalov_Nikolay', 'Shestakova_Anna', 'Skachkov_Nikolay', 'Solotky_Michael', 'Somov_Ivan', 'Tsypin_Artem', 'Zakharenko_Vadim']\n"
     ]
    }
   ],
   "source": [
    "print(sorted(data.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = train.StratifiedBatcherPreprocessed(data, 64, 0.75)\n",
    "# batch_sampler = train.NameBatcher(data, 64, train_problems, test_problems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dragunov_Nikita',\n",
       " 'Dzhenakov_Dmitriy',\n",
       " 'Filimonov_Vladislav',\n",
       " 'Fominskaya_Galina',\n",
       " 'Iavorskaia_Maria',\n",
       " 'Medvedev_Alex',\n",
       " 'Medvedev_Dmitry',\n",
       " 'Poletaev_Vsevolod',\n",
       " 'Ponomareva_Lubov',\n",
       " 'Rudnev_Viktor',\n",
       " 'Sagaydak_Oleg',\n",
       " 'Shamshiev_Mamat',\n",
       " 'Shatalov_Nikolay',\n",
       " 'Shestakova_Anna',\n",
       " 'Skachkov_Nikolay',\n",
       " 'Solotky_Michael',\n",
       " 'Somov_Ivan',\n",
       " 'Tsypin_Artem',\n",
       " 'Zakharenko_Vadim']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sampler.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_sampler.get_n_classes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18]),\n",
       " array([15, 13, 17, 17, 18, 15, 18, 14, 11, 17, 17, 15,  6, 17, 17, 21, 12,\n",
       "        15,  8]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(batch_sampler.y_train, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "        17, 18]),\n",
       " array([6, 5, 6, 6, 6, 6, 7, 5, 4, 6, 6, 6, 3, 6, 6, 8, 5, 6, 3]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(batch_sampler.y_test, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "def build_confusion_matrix(predicted_probs, true):\n",
    "    n_labels = predicted_probs.shape[1]\n",
    "#     assert true_y.shape == predicted_probs.shape\n",
    "    result = np.zeros(shape=(n_labels, n_labels))\n",
    "    \n",
    "    pred = predicted_probs.argmax(axis=1)\n",
    "#     true = true_y.argmax(axis=1)\n",
    "    \n",
    "    for pred_cls in range(n_labels):\n",
    "        for true_cls in range(n_labels):\n",
    "            result[true_cls, pred_cls] = np.count_nonzero(true[pred == pred_cls] == true_cls)\n",
    "    norm = result.sum(axis=1)\n",
    "    norm = np.maximum(norm, 1)\n",
    "#     result /= norm[:, None]\n",
    "    return result\n",
    "\n",
    "def plot_confusion_matrix(confusion_matrix):\n",
    "    fig = plt.figure( figsize=(20, 20))\n",
    "    plt.xlabel(\"True classes\")\n",
    "    plt.ylabel(\"Predicted classes\")\n",
    "#     sns.heatmap(confusion_matrix, annot=True, vmin=0.0, vmax=1.0, cmap=\"YlGnBu\")\n",
    "    sns.heatmap(confusion_matrix, annot=True, vmin=0.0, cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depth(root):\n",
    "    def dfs(node, depth=0):\n",
    "        res = depth\n",
    "        for child in ast.iter_child_nodes(node):\n",
    "            res = max(res, dfs(child, depth + 1))\n",
    "        return res\n",
    "    \n",
    "    return dfs(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_breadth(root):\n",
    "    def dfs(node):\n",
    "        res = len(list(ast.iter_child_nodes(node)))\n",
    "        for child in ast.iter_child_nodes(node):\n",
    "            res = max(dfs(child), res)\n",
    "        return res\n",
    "    \n",
    "    return dfs(root)\n",
    "\n",
    "def get_breadth(root):\n",
    "    def dfs(node, depth=0):\n",
    "        res = len(list(ast.iter_child_nodes(node)))\n",
    "        if depth == 6:\n",
    "            return res\n",
    "        for child in ast.iter_child_nodes(node):\n",
    "            z = dfs(child, depth + 1)\n",
    "            if z != -100:\n",
    "                return res\n",
    "            \n",
    "        return -100\n",
    "    \n",
    "    return dfs(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe(x):\n",
    "    def desc_f(x, f): \n",
    "        z = list(map(f, x))\n",
    "        print((np.mean(z), np.min(z), np.max(z), np.median(z)))\n",
    "        plt.hist(z)\n",
    "        plt.show()\n",
    "        \n",
    "    desc_f(x, get_depth)\n",
    "    desc_f(x, get_breadth)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n",
      "Failed\n"
     ]
    }
   ],
   "source": [
    "data = train.read_all_anytask()\n",
    "for handle, result_for_handle in data.items():\n",
    "    current = {}\n",
    "    for problem, solution in result_for_handle.items():\n",
    "#         if problem in common_problems:\n",
    "            if not fails(lambda: ast.parse(solution)):\n",
    "                current[problem] = solution\n",
    "            else:\n",
    "                print(\"Failed\")\n",
    "    \n",
    "    data[handle] = current\n",
    "batch_sampler = train.StratifiedBatcherPreprocessed(data, 64, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11.006134969325153, 3, 20, 11.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.1349693251533743, -100, 103, 2.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = batch_sampler.x_train, batch_sampler.y_train\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11.487179487179487, 4, 19, 11.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.8803418803418803, -100, 17, 2.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = batch_sampler.x_test, batch_sampler.y_test\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10623"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train.read_all()\n",
    "data = {handle:result_for_handle for handle, result_for_handle in data.items() if len(result_for_handle) >= 55 and len(result_for_handle) <= 70}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "for handle, result_for_handle in data.items():\n",
    "    current = {}\n",
    "    for problem, solution in result_for_handle.items():\n",
    "#         if problem in common_problems:\n",
    "            if not fails(lambda: ast.parse(solution)):\n",
    "                current[problem] = solution\n",
    "            else:\n",
    "                print(\"Failed\")\n",
    "    \n",
    "    data[handle] = current"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_sampler = train.StratifiedBatcherPreprocessed(data, 64, 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8.9581589958159, 0, 18, 9.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6.050209205020921, -100, 32, 6.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEFZJREFUeJzt3X+sX3V9x/Hna1TY1M2CvXZdS3brrG7M/ZBdCYvuFzhFNJYljmDMrErSzDGHPxIskoy/TECNTJPNpQNmyQjIEEczcYoMZpaM6gX5XZGKIK2FXqOimwmu870/vgf3Xe293/Z77u399uPzkXzzPedzPud73vf0fl899/M953xTVUiS2vVTy12AJGlpGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxq1Y7gIAVq1aVdPT08tdhiQdVe64445vVtXUqH4TEfTT09PMzs4udxmSdFRJ8uih9HPoRpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGjcRV8ZKEsD0lk8ty3YfueQ1y7LdI8UjeklqnEEvSY0z6CWpcQa9JDVuZNAnuTLJviT3HWTZu5NUklXdfJJ8JMmuJPckOXkpipYkHbpDOaL/GHDGgY1JTgReCXx9qPnVwIbusRn4aP8SJUl9jAz6qvo88K2DLLoMuACoobaNwFU1cDuwMsmaRalUkjSWscbok2wE9lTV3QcsWgs8NjS/u2s72GtsTjKbZHZubm6cMiRJh+Cwgz7JM4H3An/ZZ8NVtbWqZqpqZmpq5FceSpLGNM6Vsb8ErAfuTgKwDrgzySnAHuDEob7rujZJ0jI57CP6qrq3qp5XVdNVNc1geObkqnoc2A68qTv75lTgyarau7glS5IOx6GcXnkN8B/Ai5LsTnLuAt1vAh4GdgF/B/zZolQpSRrbyKGbqnrDiOXTQ9MFnNe/LEnSYvHKWElqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxh/Ll4Fcm2ZfkvqG2DyT5cpJ7knwyycqhZRcm2ZXkwSSvWqrCJUmH5lCO6D8GnHFA283Ai6vq14GvABcCJDkJOAf41W6dv0lyzKJVK0k6bCODvqo+D3zrgLbPVtX+bvZ2YF03vRG4tqqeqqqvAbuAUxaxXknSYVqMMfq3Ap/uptcCjw0t2921SZKWSa+gT3IRsB+4eox1NyeZTTI7NzfXpwxJ0gLGDvokbwZeC7yxqqpr3gOcONRtXdf2Y6pqa1XNVNXM1NTUuGVIkkYYK+iTnAFcALyuqr4/tGg7cE6S45KsBzYAX+hfpiRpXCtGdUhyDfD7wKoku4GLGZxlcxxwcxKA26vqT6vq/iTXAQ8wGNI5r6r+Z6mKlySNNjLoq+oNB2m+YoH+7wPe16coSdLi8cpYSWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEjgz7JlUn2JblvqO2EJDcneah7Pr5rT5KPJNmV5J4kJy9l8ZKk0Q7liP5jwBkHtG0BbqmqDcAt3TzAq4EN3WMz8NHFKVOSNK6RQV9Vnwe+dUDzRmBbN70NOGuo/aoauB1YmWTNYhUrSTp8447Rr66qvd3048Dqbnot8NhQv91dmyRpmfT+MLaqCqjDXS/J5iSzSWbn5ub6liFJmse4Qf/E00My3fO+rn0PcOJQv3Vd24+pqq1VNVNVM1NTU2OWIUkaZdyg3w5s6qY3ATcOtb+pO/vmVODJoSEeSdIyWDGqQ5JrgN8HViXZDVwMXAJcl+Rc4FHg7K77TcCZwC7g+8BblqBmSdJhGBn0VfWGeRadfpC+BZzXtyhJ0uLxylhJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNW7kTc0k/WSZ3vKp5S5Bi8wjeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGtcr6JO8M8n9Se5Lck2Sn06yPsmOJLuSfDzJsYtVrCTp8I0d9EnWAn8BzFTVi4FjgHOAS4HLquoFwLeBcxejUEnSePoO3awAfibJCuCZwF7gNOD6bvk24Kye25Ak9TB20FfVHuCDwNcZBPyTwB3Ad6pqf9dtN7D2YOsn2ZxkNsns3NzcuGVIkkboM3RzPLARWA/8AvAs4IxDXb+qtlbVTFXNTE1NjVuGJGmEPkM3rwC+VlVzVfXfwA3Ay4CV3VAOwDpgT88aJUk99An6rwOnJnlmkgCnAw8AtwKv7/psAm7sV6IkqY8+Y/Q7GHzoeidwb/daW4H3AO9Ksgt4LnDFItQpSRpTr/vRV9XFwMUHND8MnNLndSVJi8crYyWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TG9Qr6JCuTXJ/ky0l2JvntJCckuTnJQ93z8YtVrCTp8PU9ov8w8C9V9cvAbwA7gS3ALVW1Abilm5ckLZOxgz7Jc4DfBa4AqKofVNV3gI3Atq7bNuCsvkVKksbX54h+PTAH/H2SLyW5PMmzgNVVtbfr8ziwum+RkqTx9Qn6FcDJwEer6iXAf3HAME1VFVAHWznJ5iSzSWbn5uZ6lCFJWkifoN8N7K6qHd389QyC/4kkawC6530HW7mqtlbVTFXNTE1N9ShDkrSQsYO+qh4HHkvyoq7pdOABYDuwqWvbBNzYq0JJUi8req7/duDqJMcCDwNvYfCfx3VJzgUeBc7uuQ1JUg+9gr6q7gJmDrLo9D6vK0laPF4ZK0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDWud9AnOSbJl5L8cze/PsmOJLuSfLz74nBJ0jJZjCP684GdQ/OXApdV1QuAbwPnLsI2JElj6hX0SdYBrwEu7+YDnAZc33XZBpzVZxuSpH76HtH/FXAB8MNu/rnAd6pqfze/G1jbcxuSpB7GDvokrwX2VdUdY66/Oclsktm5ublxy5AkjdDniP5lwOuSPAJcy2DI5sPAyiQruj7rgD0HW7mqtlbVTFXNTE1N9ShDkrSQsYO+qi6sqnVVNQ2cA/xrVb0RuBV4fddtE3Bj7yolSWNbivPo3wO8K8kuBmP2VyzBNiRJh2jF6C6jVdVtwG3d9MPAKYvxupKk/rwyVpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDXOoJekxhn0ktQ4g16SGmfQS1LjDHpJapxBL0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjRs76JOcmOTWJA8kuT/J+V37CUluTvJQ93z84pUrSTpcfY7o9wPvrqqTgFOB85KcBGwBbqmqDcAt3bwkaZmMHfRVtbeq7uymvwfsBNYCG4FtXbdtwFl9i5QkjW9RxuiTTAMvAXYAq6tqb7focWD1POtsTjKbZHZubm4xypAkHUTvoE/ybOATwDuq6rvDy6qqgDrYelW1tapmqmpmamqqbxmSpHn0Cvokz2AQ8ldX1Q1d8xNJ1nTL1wD7+pUoSeqjz1k3Aa4AdlbVh4YWbQc2ddObgBvHL0+S1NeKHuu+DPgT4N4kd3Vt7wUuAa5Lci7wKHB2vxIlSX2MHfRV9e9A5ll8+rivK0laXF4ZK0mNM+glqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWqcQS9JjTPoJalxBr0kNc6gl6TGGfSS1DiDXpIaZ9BLUuMMeklqnEEvSY0z6CWpcQa9JDWuz5eDLyjJGcCHgWOAy6vqkqXaliT1Mb3lU8u27Ucuec2Sb2NJgj7JMcBfA38I7Aa+mGR7VT2w2Ntq/R9IkvpaqqGbU4BdVfVwVf0AuBbYuETbkiQtYKmCfi3w2ND87q5NknSELdkY/ShJNgObu9n/TPLgmC+1Cvjm4lR1eHJp75dYttp7su4jy7qPrCNad88c+cVD6bRUQb8HOHFofl3X9iNVtRXY2ndDSWaraqbv6yyHo7V26z6yrPvIOlrrXshSDd18EdiQZH2SY4FzgO1LtC1J0gKW5Ii+qvYn+XPgMwxOr7yyqu5fim1Jkha2ZGP0VXUTcNNSvf6Q3sM/y+hord26jyzrPrKO1rrnlapa7hokSUvIWyBIUuOOqqBP8sdJ7k/ywyQzByy7MMmuJA8medVQ+xld264kW4581f9fkt9McnuSu5LMJjmla0+Sj3R13pPk5OWu9UBJ3p7ky92/wfuH2g+67ydJkncnqSSruvmJ3t9JPtDt63uSfDLJyqFlE72/J+09t5AkJya5NckD3e/1+V37CUluTvJQ93z8ctfaS1UdNQ/gV4AXAbcBM0PtJwF3A8cB64GvMvgQ+Jhu+vnAsV2fk5b5Z/gs8Opu+kzgtqHpTwMBTgV2LPf+PqDuPwA+BxzXzT9voX2/3PUeUPuJDE4MeBRYdZTs71cCK7rpS4FLj4b9PYnvuRH1rgFO7qZ/FvhKt4/fD2zp2rc8vf+P1sdRdURfVTur6mAXVm0Erq2qp6rqa8AuBrdhmMRbMRTwc930c4BvdNMbgatq4HZgZZI1y1HgPN4GXFJVTwFU1b6ufb59P0kuAy5gsO+fNtH7u6o+W1X7u9nbGVyLApO/vyfxPTevqtpbVXd2098DdjK4in8jsK3rtg04a3kqXBxHVdAvYL5bLkzirRjeAXwgyWPAB4ELu/ZJrHXYC4HfSbIjyb8leWnXPtF1J9kI7Kmquw9YNNF1H+CtDP76gMmve9Lrm1eSaeAlwA5gdVXt7RY9DqxeprIWxbLdAmE+ST4H/PxBFl1UVTce6XrGsdDPAJwOvLOqPpHkbOAK4BVHsr75jKh7BXACg2GOlwLXJXn+ESxvXiPqfi+DYZCJcyi/60kuAvYDVx/J2n7SJHk28AngHVX13SQ/WlZVleSoPj1x4oK+qsYJvYVuubDgrRiWwkI/Q5KrgPO72X8ELu+mR942YqmNqPttwA01GLT8QpIfMrgnyMTWneTXGIxj3929cdcBd3YfgE9s3U9L8mbgtcDp3X6HCah7hEmv78ckeQaDkL+6qm7omp9Isqaq9nZDevvmf4XJ18rQzXbgnCTHJVkPbAC+wGTeiuEbwO9106cBD3XT24E3dWeDnAo8OfSn4yT4JwYfyJLkhQw+aPsm8+/7ZVdV91bV86pquqqmGQwjnFxVjzPh+zuDL+65AHhdVX1/aNHE7u/OJL7n5pXBEcAVwM6q+tDQou3Apm56E3BUjCbMa7k/DT6cB/BHDN6sTwFPAJ8ZWnYRg0/7H6Q7q6VrP5PBJ+lfZfAn8XL/DC8H7mBwNsIO4Le69jD4spavAvcydFbRJDwYBPs/APcBdwKnjdr3k/YAHuH/zrqZ9P29i8FY913d42+Plv09ae+5EbW+nMGH9PcM7eszgecCtzA4EPsccMJy19rn4ZWxktS4VoZuJEnzMOglqXEGvSQ1zqCXpMYZ9JLUOINekhpn0EtS4wx6SWrc/wJiSKN6pIm0PQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = batch_sampler.x_train, batch_sampler.y_train\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9.072289156626505, 5, 18, 9.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5.481927710843373, -100, 39, 6.0)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEPpJREFUeJzt3X2MZXV9x/H3p6z4gFaexu2Wxe4aEYttBB0JBtsoCxbBuNvEEkzTbFuSTW1rQU101T9Mm/4Baoo2adpsxHZMUZ6ELEGr4hbaNKmLszwJrHQXBIUuu6OF+tAEi3z7xz2UYZnh3pm5d+/ll/crubnn/M65cz57svczZ865D6kqJEnPf78w7gCSpOGw0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNWHUoN3bsscfWunXrDuUmJel5b9euXT+oqql+6x3SQl+3bh2zs7OHcpOS9LyX5MFB1hvolEuS9ye5O8ldSb6Y5EVJ1ifZmWRvkiuTHL6yyJKklehb6EmOA/4MmK6qXwMOA84HLgEurapXA48CF4wyqCTpuQ16UXQV8OIkq4CXAPuAM4BruuUzwKbhx5MkDapvoVfVw8CngO/RK/L/BnYBj1XVE91qDwHHLfT4JFuSzCaZnZubG05qSdKzDHLK5ShgI7Ae+GXgCODsQTdQVduqarqqpqem+l6klSQt0yCnXM4EvltVc1X1v8C1wOnAkd0pGIC1wMMjyihJGsAghf494LQkL0kSYANwD3AT8O5unc3A9tFElCQNYpBz6DvpXfy8Ffh295htwIeBDyTZCxwDXDbCnJKkPgZ6Y1FVfRz4+EHD9wOnDj2RJGlZDuk7RSVNjnVbvzy2bT9w8blj23bL/HAuSWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1Ij+hZ6khOT3D7v9qMkFyU5OsmNSfZ090cdisCSpIUN8p2i91bVyVV1MvBG4H+A64CtwI6qOgHY0c1LksZkqadcNgD3VdWDwEZgphufATYNM5gkaWmWWujnA1/spldX1b5u+hFg9dBSSZKWbOBCT3I48C7g6oOXVVUBtcjjtiSZTTI7Nze37KCSpOe2lCP0dwC3VtX+bn5/kjUA3f2BhR5UVduqarqqpqemplaWVpK0qKUU+nt4+nQLwPXA5m56M7B9WKEkSUs3UKEnOQI4C7h23vDFwFlJ9gBndvOSpDFZNchKVfVT4JiDxn5I71UvkqQJ4DtFJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREWuiQ1YtDvFD0yyTVJvpNkd5I3Jzk6yY1J9nT3R406rCRpcYMeoX8G+GpVvRZ4PbAb2ArsqKoTgB3dvCRpTPoWepKXA78JXAZQVT+rqseAjcBMt9oMsGlUISVJ/Q1yhL4emAP+PsltST6b5AhgdVXt69Z5BFg9qpCSpP4GKfRVwBuAv62qU4CfctDplaoqoBZ6cJItSWaTzM7Nza00ryRpEYMU+kPAQ1W1s5u/hl7B70+yBqC7P7DQg6tqW1VNV9X01NTUMDJLkhbQt9Cr6hHg+0lO7IY2APcA1wObu7HNwPaRJJQkDWTVgOu9D7g8yeHA/cAf0PtlcFWSC4AHgfNGE1GSNIiBCr2qbgemF1i0YbhxJEnL5TtFJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqREDfWNRkgeAHwM/B56oqukkRwNXAuuAB4DzqurR0cSUJPWzlCP0t1XVyVX11FfRbQV2VNUJwI5uXpI0Jis55bIRmOmmZ4BNK48jSVquQQu9gK8n2ZVkSze2uqr2ddOPAKuHnk6SNLCBzqEDb6mqh5O8ArgxyXfmL6yqSlILPbD7BbAF4JWvfOWKwkqSFjfQEXpVPdzdHwCuA04F9idZA9DdH1jksduqarqqpqempoaTWpL0LH0LPckRSV721DTwduAu4Hpgc7faZmD7qEJKkvob5JTLauC6JE+t/4Wq+mqSbwFXJbkAeBA4b3QxJUn99C30qrofeP0C4z8ENowilCRp6XynqCQ1wkKXpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRgxc6EkOS3Jbkhu6+fVJdibZm+TKJIePLqYkqZ+lHKFfCOyeN38JcGlVvRp4FLhgmMEkSUszUKEnWQucC3y2mw9wBnBNt8oMsGkUASVJgxn0CP3TwIeAJ7v5Y4DHquqJbv4h4LghZ5MkLUHfQk/yTuBAVe1azgaSbEkym2R2bm5uOT9CkjSAQY7QTwfeleQB4Ap6p1o+AxyZZFW3zlrg4YUeXFXbqmq6qqanpqaGEFmStJC+hV5VH6mqtVW1Djgf+Oeq+l3gJuDd3Wqbge0jSylJ6mslr0P/MPCBJHvpnVO/bDiRJEnLsar/Kk+rqpuBm7vp+4FThx9JkrQcvlNUkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGtG30JO8KMktSe5IcneSP+/G1yfZmWRvkiuTHD76uJKkxQxyhP44cEZVvR44GTg7yWnAJcClVfVq4FHggtHFlCT107fQq+cn3ewLulsBZwDXdOMzwKaRJJQkDWSgc+hJDktyO3AAuBG4D3isqp7oVnkIOG40ESVJgxio0Kvq51V1MrAWOBV47aAbSLIlyWyS2bm5uWXGlCT1s6RXuVTVY8BNwJuBI5Os6hatBR5e5DHbqmq6qqanpqZWFFaStLhBXuUyleTIbvrFwFnAbnrF/u5utc3A9lGFlCT1t6r/KqwBZpIcRu8XwFVVdUOSe4ArkvwlcBtw2QhzSpL66FvoVXUncMoC4/fTO58uSZoAvlNUkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjBvmS6OOT3JTkniR3J7mwGz86yY1J9nT3R40+riRpMYMcoT8BfLCqTgJOA/4kyUnAVmBHVZ0A7OjmJUlj0rfQq2pfVd3aTf8Y2A0cB2wEZrrVZoBNowopSepvSefQk6wDTgF2Aqural+36BFg9VCTSZKWZOBCT/JS4EvARVX1o/nLqqqAWuRxW5LMJpmdm5tbUVhJ0uIGKvQkL6BX5pdX1bXd8P4ka7rla4ADCz22qrZV1XRVTU9NTQ0jsyRpAYO8yiXAZcDuqvqreYuuBzZ305uB7cOPJ0ka1KoB1jkd+D3g20lu78Y+ClwMXJXkAuBB4LzRRJQkDaJvoVfVvwFZZPGG4caRJC2X7xSVpEZY6JLUCAtdkhphoUtSIyx0SWqEhS5JjbDQJakRFrokNcJCl6RGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktSIQb4k+nNJDiS5a97Y0UluTLKnuz9qtDElSf0McoT+D8DZB41tBXZU1QnAjm5ekjRGfQu9qv4V+K+DhjcCM930DLBpyLkkSUu03HPoq6tqXzf9CLB6sRWTbEkym2R2bm5umZuTJPWz4ouiVVVAPcfybVU1XVXTU1NTK92cJGkRyy30/UnWAHT3B4YXSZK0HMst9OuBzd30ZmD7cOJIkpZrkJctfhH4d+DEJA8luQC4GDgryR7gzG5ekjRGq/qtUFXvWWTRhiFnkSStgO8UlaRGWOiS1AgLXZIaYaFLUiMsdElqhIUuSY2w0CWpERa6JDXCQpekRljoktQIC12SGmGhS1IjLHRJakTfT1uUpGFbt/XLY9nuAxefO5btHioeoUtSIyx0SWqEhS5JjVhRoSc5O8m9SfYm2TqsUJKkpVv2RdEkhwF/A5wFPAR8K8n1VXXPsMLN50UUSSvVeo+s5Aj9VGBvVd1fVT8DrgA2DieWJGmpVlLoxwHfnzf/UDcmSRqDkb8OPckWYEs3+5Mk9y7zRx0L/GA4qQaXS5b90LHkXYHnU97nU1Yw76hNfN6DemQ5eX9lkJVWUugPA8fPm1/bjT1DVW0Dtq1gOwAkma2q6ZX+nEPFvKPzfMoK5h018z5tJadcvgWckGR9ksOB84HrhxNLkrRUyz5Cr6onkvwp8DXgMOBzVXX30JJJkpZkRefQq+orwFeGlKWfFZ+2OcTMOzrPp6xg3lEzbydVNaqfLUk6hHzrvyQ1YiILPcnvJLk7yZNJpg9a9pHuowbuTfJb88Yn4mMIkpyc5JtJbk8ym+TUbjxJ/rrLd2eSN4wr43xJ3pfkO93+/sS88QX38yRI8sEkleTYbn5S9+0nu317Z5Lrkhw5b9lE7t9JeR4tJMnxSW5Kck/3//XCbvzoJDcm2dPdHzXurPMlOSzJbUlu6ObXJ9nZ7eMruxeVDEdVTdwN+FXgROBmYHre+EnAHcALgfXAffQuyB7WTb8KOLxb56QxZf868I5u+hzg5nnT/wQEOA3YOQH7+W3AN4AXdvOveK79PO68Xbbj6V2IfxA4dlL3bZfr7cCqbvoS4JJJ3r+T9DxaJN8a4A3d9MuA/+j25SeArd341qf286TcgA8AXwBu6OavAs7vpv8OeO+wtjWRR+hVtbuqFnoD0kbgiqp6vKq+C+yl9xEEk/QxBAX8Yjf9cuA/u+mNwOer55vAkUnWjCPgPO8FLq6qxwGq6kA3vth+ngSXAh+it5+fMon7lqr6elU90c1+k957NWBy9+8kPY+epar2VdWt3fSPgd303p2+EZjpVpsBNo0n4bMlWQucC3y2mw9wBnBNt8pQ805koT+HxT5uYJI+huAi4JNJvg98CvhINz5JGZ/yGuA3uj///iXJm7rxScxKko3Aw1V1x0GLJjLvQf6Q3l8RMLl5JzXXsyRZB5wC7ARWV9W+btEjwOoxxVrIp+kdgDzZzR8DPDbvF/1Q9/HYvoIuyTeAX1pg0ceqavuhzrMUz5Ud2AC8v6q+lOQ84DLgzEOZb74+WVcBR9M7TfEm4KokrzqE8Z6lT96P0juNMTEG+X+c5GPAE8DlhzJbq5K8FPgScFFV/ah30NtTVZVkIl66l+SdwIGq2pXkrYdim2Mr9KpaTsk918cN9P0YgmF5ruxJPg9c2M1eTfenFgN+VMKw9cn6XuDa6p3MuyXJk/Q+Z2IsWWHxvEl+nd755ju6J/Ba4NbuovPE5X1Kkt8H3gls6PYzjDFvH5Oa6/8leQG9Mr+8qq7thvcnWVNV+7pTbQcW/wmH1OnAu5KcA7yI3qnYz9A7JbiqO0of7j4e9wWDPhcTbuaZF0VfxzMvJt1P70LOqm56PU9fzHndmDLvBt7aTW8AdnXT5/LMC3e3TMD+/SPgL7rp19D7czuL7edx5z0o+wM8fVF04vZtl+ts4B5g6qDxidy/k/Q8WiRfgM8Dnz5o/JM886LoJ8addYHsb+Xpi6JX88yLon88tO2M+x+6yD/+t+mdW3oc2A98bd6yj9G7En8v3atJuvFz6F31vo/en7vjyv4WYFf3ZNgJvLEbD70vBLkP+Pb8X1RjzHo48I/AXcCtwBn99vOk3A4q9Inbt12uvd0vydu7299N+v6dlOfRItneQu9i+J3z9uk59M5L7wD20HvV1tHjzrpA9vmF/irglu7/x9V0rzIbxs13ikpSI55vr3KRJC3CQpekRljoktQIC12SGmGhS1IjLHRJaoSFLkmNsNAlqRH/B4joA/pbLABwAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x, y = batch_sampler.x_test, batch_sampler.y_test\n",
    "describe(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
